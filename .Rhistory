num_predicted_satellites <- sum(result$PrunedTree_Predicted == 1)
print("Numero di crabs predetti avere i satelliti con l'albero potato:")
print(num_predicted_satellites)
# Generate data
data <- rep(0, 25)
# Likelihood function for Poisson via binomial
lik_bin <- function(theta, data){
dbinom(data, length(data), theta)
}
# Beta prior
prior_beta <- function(theta, alpha, beta){
dbeta(theta, alpha, beta)
}
# Normal prior
prior_norm <- function(theta, mu, sigma){
dnorm(theta, mean = mu, sd = sigma)
}
# Plotting
curve(lik_bin(x, data), from = 0, to = 1, n = 25,
xlab = expression(theta), ylab = "density", lwd = 2,
col = "black", main = "Likelihood and Priors")
curve(prior_beta(x, 0.5, 0.5), from = 0, to = 1, n = 25,
lty = 2, col = "red", add = TRUE, lwd = 2)
curve(prior_beta(x, 1, 1), from = 0, to = 1, n = 25,
lty = 3, col = "blue", add = TRUE, lwd = 2)
curve(prior_beta(x, 10, 10), from = 0, to = 1, n = 25,
lty = 4, col = "green", add = TRUE, lwd = 2)
curve(prior_norm(x, 0, 1), from = 0, to = 1, n = 25,
lty = 5, col = "violet", add = TRUE, lwd = 2)
legend(2.6, 1.8, c("Likelihood", "Beta(0.5,0.5)", "Beta(1,1)", "Beta(10,10)","N(0, 1)" ),
lty = c(1, 2, 3, 4, 5), col = c("black", "red", "blue", "green", "violet"),
lwd = 2, cex = 0.9)
# Bayesian estimation
bayesian_estimate <- function(data, alpha, beta) {
posterior_alpha <- sum(data) + alpha
posterior_beta <- length(data) - sum(data) + beta
posterior_mean <- posterior_alpha / (posterior_alpha + posterior_beta)
return(posterior_mean)
}
# Calculate Bayesian estimates
posterior_mean_0.5 <- bayesian_estimate(data, 0.5, 0.5)
posterior_mean_1.0 <- bayesian_estimate(data, 1.0, 1.0)
posterior_mean_10.0 <- bayesian_estimate(data, 10.0, 10.0)
# Generate data
data <- rep(0, 25)
# Likelihood function for Poisson via binomial
lik_bin <- function(theta, data){
dbinom(data, length(data), theta)
}
# Beta prior
prior_beta <- function(theta, alpha, beta){
dbeta(theta, alpha, beta)
}
# Normal prior
prior_norm <- function(theta, mu, sigma){
dnorm(theta, mean = mu, sd = sigma)
}
# Plotting
curve(lik_bin(x, data), from = 0, to = 1, n = 25,
xlab = expression(theta), ylab = "density", lwd = 2,
col = "black", main = "Likelihood and Priors")
curve(prior_beta(x, 0.5, 0.5), from = 0, to = 1, n = 25,
lty = 2, col = "red", add = TRUE, lwd = 2)
curve(prior_beta(x, 1, 1), from = 0, to = 1, n = 25,
lty = 3, col = "blue", add = TRUE, lwd = 2)
curve(prior_beta(x, 10, 10), from = 0, to = 1, n = 25,
lty = 4, col = "green", add = TRUE, lwd = 2)
curve(prior_norm(x, 0, 1), from = 0, to = 1, n = 25,
lty = 5, col = "violet", add = TRUE, lwd = 2)
# Plotting
curve(lik_bin(x, data), from = 0, to = 1, n = 25,
xlab = expression(theta), ylab = "density", lwd = 2,
col = "black", main = "Likelihood and Priors")
library(ggplot2)
# Generate data
data <- rep(0, 25)
# Likelihood function for Poisson via binomial
lik_bin <- function(theta, data){
dbinom(data, length(data), theta)
}
# Beta prior
prior_beta <- function(theta, alpha, beta){
dbeta(theta, alpha, beta)
}
# Normal prior
prior_norm <- function(theta, mu, sigma){
dnorm(theta, mean = mu, sd = sigma)
}
# Plotting
curve(lik_bin(x, data), from = 0, to = 1, n = 25,
xlab = expression(theta), ylab = "density", lwd = 2,
col = "black", main = "Likelihood and Priors")
curve(prior_beta(x, 0.5, 0.5), from = 0, to = 1, n = 25,
lty = 2, col = "red", add = TRUE, lwd = 2)
curve(prior_beta(x, 1, 1), from = 0, to = 1, n = 25,
lty = 3, col = "blue", add = TRUE, lwd = 2)
# Beta prior
prior_beta <- function(theta, alpha, beta){
dbeta(theta, alpha, beta)
}
# Normal prior
prior_norm <- function(theta, mu, sigma){
dnorm(theta, mean = mu, sd = sigma)
}
# Plotting
curve(lik_bin(x, data), from = 0, to = 1, n = 25,
xlab = expression(theta), ylab = "density", lwd = 2,
col = "black", main = "Likelihood and Priors")
curve(prior_beta(x, 0.5, 0.5), from = 0, to = 1, n = 25,
lty = 2, col = "red", add = TRUE, lwd = 2)
curve(prior_beta(x, 1, 1), from = 0, to = 1, n = 25,
lty = 3, col = "blue", add = TRUE, lwd = 2)
curve(prior_beta(x, 10, 10), from = 0, to = 1, n = 25,
lty = 4, col = "green", add = TRUE, lwd = 2)
curve(prior_norm(x, 0, 1), from = 0, to = 1, n = 25,
lty = 5, col = "violet", add = TRUE, lwd = 2)
legend(2.6, 1.8, c("Likelihood", "Beta(0.5,0.5)", "Beta(1,1)", "Beta(10,10)","N(0, 1)" ),
lty = c(1, 2, 3, 4, 5), col = c("black", "red", "blue", "green", "violet"),
lwd = 2, cex = 0.9)
# Normal prior
prior_norm <- function(theta, mu, sigma){
dnorm(theta, mean = mu, sd = sigma)
}
# Plotting
curve(lik_bin(x, data), from = 0, to = 1, n = 25,
xlab = expression(theta), ylab = "density", lwd = 2,
col = "black", main = "Likelihood and Priors")
curve(prior_beta(x, 0.5, 0.5), from = 0, to = 1, n = 25,
lty = 2, col = "red", add = TRUE, lwd = 2)
curve(prior_beta(x, 1, 1), from = 0, to = 1, n = 25,
lty = 3, col = "blue", add = TRUE, lwd = 2)
library(ggplot2)
# Generate data
data <- rep(0, 25)
# Likelihood function for Poisson via binomial
lik_bin <- function(theta, data){
dbinom(data, length(data), theta)
}
# Beta prior
prior_beta <- function(theta, alpha, beta){
dbeta(theta, alpha, beta)
}
# Normal prior
prior_norm <- function(theta, mu, sigma){
dnorm(theta, mean = mu, sd = sigma)
}
# Plotting
curve(lik_bin(x, data), from = 0, to = 1, n = 25,
xlab = expression(theta), ylab = "density", lwd = 2,
col = "black", main = "Likelihood and Priors")
curve(prior_beta(x, 0.5, 0.5), from = 0, to = 1, n = 25,
lty = 2, col = "red", add = TRUE, lwd = 2)
curve(prior_beta(x, 1, 1), from = 0, to = 1, n = 25,
lty = 3, col = "blue", add = TRUE, lwd = 2)
curve(prior_beta(x, 10, 10), from = 0, to = 1, n = 25,
lty = 4, col = "green", add = TRUE, lwd = 2)
curve(prior_norm(x, 0, 1), from = 0, to = 1, n = 25,
lty = 5, col = "violet", add = TRUE, lwd = 2)
legend(2.6, 1.8, c("Likelihood", "Beta(0.5,0.5)", "Beta(1,1)", "Beta(10,10)","N(0, 1)" ),
lty = c(1, 2, 3, 4, 5), col = c("black", "red", "blue", "green", "violet"),
lwd = 2, cex = 0.9)
# Bayesian estimation
bayesian_estimate <- function(data, alpha, beta) {
posterior_alpha <- sum(data) + alpha
posterior_beta <- length(data) - sum(data) + beta
posterior_mean <- posterior_alpha / (posterior_alpha + posterior_beta)
return(posterior_mean)
}
# Calculate Bayesian estimates
posterior_mean_0.5 <- bayesian_estimate(data, 0.5, 0.5)
posterior_mean_1.0 <- bayesian_estimate(data, 1.0, 1.0)
posterior_mean_10.0 <- bayesian_estimate(data, 10.0, 10.0)
library(ggplot2)
library(mgcv)
# Generate data
data <- rep(0, 25)
# Likelihood function for Poisson via binomial
lik_bin <- function(theta, data){
dbinom(data, length(data), theta)
}
# Beta prior
prior_beta <- function(theta, alpha, beta){
dbeta(theta, alpha, beta)
}
# Normal prior
prior_norm <- function(theta, mu, sigma){
dnorm(theta, mean = mu, sd = sigma)
}
# Plotting
curve(lik_bin(x, data), from = 0, to = 1, n = 25,
xlab = expression(theta), ylab = "density", lwd = 2,
col = "black", main = "Likelihood and Priors")
curve(prior_beta(x, 0.5, 0.5), from = 0, to = 1, n = 25,
lty = 2, col = "red", add = TRUE, lwd = 2)
curve(prior_beta(x, 1, 1), from = 0, to = 1, n = 25,
lty = 3, col = "blue", add = TRUE, lwd = 2)
curve(prior_beta(x, 10, 10), from = 0, to = 1, n = 25,
lty = 4, col = "green", add = TRUE, lwd = 2)
curve(prior_norm(x, 0, 1), from = 0, to = 1, n = 25,
lty = 5, col = "violet", add = TRUE, lwd = 2)
legend(2.6, 1.8, c("Likelihood", "Beta(0.5,0.5)", "Beta(1,1)", "Beta(10,10)","N(0, 1)" ),
lty = c(1, 2, 3, 4, 5), col = c("black", "red", "blue", "green", "violet"),
lwd = 2, cex = 0.9)
# Bayesian estimation
bayesian_estimate <- function(data, alpha, beta) {
posterior_alpha <- sum(data) + alpha
posterior_beta <- length(data) - sum(data) + beta
posterior_mean <- posterior_alpha / (posterior_alpha + posterior_beta)
return(posterior_mean)
}
# Calculate Bayesian estimates
posterior_mean_0.5 <- bayesian_estimate(data, 0.5, 0.5)
posterior_mean_1.0 <- bayesian_estimate(data, 1.0, 1.0)
posterior_mean_10.0 <- bayesian_estimate(data, 10.0, 10.0)
dnorm(theta, mean = mu, sd = sigma)}
# Normal prior
prior_norm <- function(theta, mu, sigma){dnorm(theta, mean = mu, sd = sigma)}
# Plotting
curve(lik_bin(x, data), from = 0, to = 1, n = 25,
xlab = expression(theta), ylab = "density", lwd = 2,
col = "black", main = "Likelihood and Priors")
curve(prior_beta(x, 0.5, 0.5), from = 0, to = 1, n = 25,
lty = 2, col = "red", add = TRUE, lwd = 2)
curve(prior_beta(x, 1, 1), from = 0, to = 1, n = 25,
lty = 3, col = "blue", add = TRUE, lwd = 2)
curve(prior_beta(x, 10, 10), from = 0, to = 1, n = 25,
lty = 4, col = "green", add = TRUE, lwd = 2)
curve(prior_norm(x, 0, 1), from = 0, to = 1, n = 25,
lty = 5, col = "violet", add = TRUE, lwd = 2)
legend(2.6, 1.8, c("Likelihood", "Beta(0.5,0.5)", "Beta(1,1)", "Beta(10,10)","N(0, 1)" ),
lty = c(1, 2, 3, 4, 5), col = c("black", "red", "blue", "green", "violet"),
lwd = 2, cex = 0.9)
# Bayesian estimation
bayesian_estimate <- function(data, alpha, beta) {
posterior_alpha <- sum(data) + alpha
posterior_beta <- length(data) - sum(data) + beta
posterior_mean <- posterior_alpha / (posterior_alpha + posterior_beta)
return(posterior_mean)
}
# Calculate Bayesian estimates
posterior_mean_0.5 <- bayesian_estimate(data, 0.5, 0.5)
posterior_mean_1.0 <- bayesian_estimate(data, 1.0, 1.0)
posterior_mean_10.0 <- bayesian_estimate(data, 10.0, 10.0)
library(ggplot2)
library(mgcv)
# Generate data
data <- rep(0, 25)
# Likelihood function for Poisson via binomial
lik_bin <- function(theta, data){
dbinom(data, length(data), theta)
}
# Beta prior
prior_beta <- function(theta, alpha, beta){
dbeta(theta, alpha, beta)
}
# Normal prior
prior_norm <- function(theta, mu, sigma){
dnorm(theta, mean = mu, sd = sigma)}
# Plotting
curve(lik_bin(x, data), from = 0, to = 1, n = 25,
xlab = expression(theta), ylab = "density", lwd = 2,
col = "black", main = "Likelihood and Priors")
curve(prior_beta(x, 0.5, 0.5), from = 0, to = 1, n = 25,
lty = 2, col = "red", add = TRUE, lwd = 2)
curve(prior_beta(x, 1, 1), from = 0, to = 1, n = 25,
lty = 3, col = "blue", add = TRUE, lwd = 2)
curve(prior_beta(x, 10, 10), from = 0, to = 1, n = 25,
lty = 4, col = "green", add = TRUE, lwd = 2)
curve(prior_norm(x, 0, 1), from = 0, to = 1, n = 25,
lty = 5, col = "violet", add = TRUE, lwd = 2)
legend(2.6, 1.8, c("Likelihood", "Beta(0.5,0.5)", "Beta(1,1)", "Beta(10,10)","N(0, 1)" ),
lty = c(1, 2, 3, 4, 5), col = c("black", "red", "blue", "green", "violet"),
lwd = 2, cex = 0.9)
# Bayesian estimation
bayesian_estimate <- function(data, alpha, beta) {
posterior_alpha <- sum(data) + alpha
posterior_beta <- length(data) - sum(data) + beta
posterior_mean <- posterior_alpha / (posterior_alpha + posterior_beta)
return(posterior_mean)
}
# Calculate Bayesian estimates
posterior_mean_0.5 <- bayesian_estimate(data, 0.5, 0.5)
posterior_mean_1.0 <- bayesian_estimate(data, 1.0, 1.0)
posterior_mean_10.0 <- bayesian_estimate(data, 10.0, 10.0)
# Load required libraries
library(MASS)
library(ggplot2)
library(mgcv)
library(splines)
# Load the Boston dataset
data(Boston)
# Fit a cubic polynomial regression
poly_fit <- lm(nox ~ poly(Boston$dis, 3), data = Boston)
# Display regression output
summary(poly_fit)
# Define the polynomial degrees
degrees <- 1:10
# Initialize a vector for RSS values
rss_values <- numeric(length(degrees))
# Set up the plot grid
par(mfrow = c(2, 5))
# Loop over polynomial degrees
for (deg in degrees) {
# Fit the polynomial model
poly_fit_i <- lm(nox ~ poly(Boston$dis, deg), data = Boston)
# Calculate and save the RSS value
rss_values[deg] <- sum(residuals(poly_fit)^2)
# Create and display the plot
plot(Boston$dis, Boston$nox, col = "blue", pch = 20, main = paste("Degree =", deg), xlab = "dis", ylab = "nox")
lines(sort(Boston$dis), predict(poly_fit_i, newdata = Boston, col = "red"))
}
# Display the RSS values
rss_values
# Reset the plot grid
par(mfrow = c(1, 1))
# Plot the results
plot(degrees, rss_values, type = "b", main = "Residual Sum of Squares vs. Polynomial Degree", xlab = "Polynomial Degree", ylab = "Residual Sum of Squares")
# Fit a cubic polynomial regression
poly_fit <- lm(nox ~ poly(Boston$dis, 3), data = Boston)
# Fit a square polynomial regression
poly_fit2 <- lm(nox ~ poly(Boston$dis, 2), data = Boston)
# Fit a  polynomial regression with grade 4
poly_fit4 <- lm(nox ~ poly(Boston$dis, 4), data = Boston)
anova_result1 <- anova(poly_fit, poly_fit2)
anova_result2 <- anova(poly_fit, poly_fit4)
# ANOVA test results for polynomial model comparison
## anova_result1
### Model 1: nox - poly(Boston$dis, 3)
### Model 2: nox - poly(Boston$dis, 2)
#### Significant difference: p-value = 4.275e-07 (*)
## anova_result2
### Model 1: nox - poly(Boston$dis, 3)
### Model 2: nox - poly(Boston$dis, 4)
#### Not significant: p-value = 0.5894
# Based on the results, we choose a model with 3 degrees.
# Fit a regression spline with four degrees of freedom
gam_fit_d <- gam(nox ~ bs(Boston$dis, df = 4), data = Boston)
# Display the summary
summary(gam_fit_d)
# Define degrees of freedom
degrees_of_freedom <- seq(3, 10, by = 1)
rss_values_gam <- numeric(length(degrees_of_freedom))
# Loop over degrees of freedom
for (df1 in degrees_of_freedom) {
gam_fit <- gam(nox ~ bs(Boston$dis, df1), data = Boston)
rss_values_gam[df1 - 2] <- sum(resid(gam_fit)^2)
}
# Plot the results
plot(degrees_of_freedom, rss_values_gam, type = "l", xlab = "Degrees of Freedom", ylab = "RSS", main = "GAM Fit")
# Initialize vector for GCV scores
gcv_scores <- numeric(length(degrees_of_freedom))
# Loop over degrees of freedom
for (i in 1:length(degrees_of_freedom)) {
df <- degrees_of_freedom[i]
gam_fit <- gam(nox ~ bs(Boston$dis, df = df), data = Boston)
gcv_scores[i] <- gam_fit$gcv.ubre
}
# Find optimal degrees of freedom
optimal_df_gam <- degrees_of_freedom[which.min(gcv_scores)]
cat("Optimal Degrees of Freedom:", optimal_df_gam, "\n")
# Fit the model with optimal degrees of freedom
gam_fit_optimal <- gam(nox ~ bs(Boston$dis, df = optimal_df_gam), data = Boston)
summary(gam_fit_optimal)
dat <- mcycle
View(dat)
plot(dat)
par(mfrow=c(2,2))
fit <- gam(accel ~ s(times, k=35), data = dat)
plot(fit, se = FALSE, residuals = TRUE, pch=19, ylab = "K=35")
library("car")
df <-sum(influence(fit))
fit2 <- lm(accel ~ poly(times, degree = df, raw = TRUE), data = dat)
termplot(fit2, partial.resid = TRUE, pch=19, ylab= "polynomial")
library("car")
df <-sum(influence(fit))
fit2 <- lm(accel ~ poly(times, degree = df, raw = TRUE), data = dat)
termplot(fit2, partial.resid = TRUE, pch=19, ylab= "polynomial")
fit3 <- gam(accel ~ s(times, k = round(df), fx=TRUE), data = dat)
plot(fit3, se = FALSE, residuals = TRUE, pch=19, ylab = "un-penalized")
fit3 <- gam(accel ~ s(times, k = round(df), fx=TRUE), data = dat)
plot(fit3, se = FALSE, residuals = TRUE, pch=19, ylab = "un-penalized")
```{r setup, echo = TRUE}
fit3 <- gam(accel ~ s(times, k = round(df), fx=TRUE), data = dat)
plot(fit3, se = FALSE, residuals = TRUE, pch=19, ylab = "un-penalized")
library("MASS")
library("mgcv")
dat <- mcycle
plot(dat)
par(mfrow=c(2,2))
fit <- gam(accel ~ s(times, k=35), data = dat)
plot(fit, se = FALSE, residuals = TRUE, pch=19, ylab = "K=35")
plot(dat)
par(mfrow=c(2,2))
fit <- gam(accel ~ s(times, k=35), data = dat)
plot(fit, se = FALSE, residuals = TRUE, pch=19, ylab = "K=35")
library("car")
df <-sum(influence(fit))
fit2 <- lm(accel ~ poly(times, degree = df, raw = TRUE), data = dat)
termplot(fit2, partial.resid = TRUE, pch=19, ylab= "polynomial")
fit3 <- gam(accel ~ s(times, k = round(df), fx=TRUE), data = dat)
plot(fit3, se = FALSE, residuals = TRUE, pch=19, ylab = "un-penalized")
library(splines)
par(mfrow=c(2,2))
fits <- lm(accel ~ bs(times, degree = 3, knots = c(15, 20, 30, 40)), data = dat)
termplot(fits, partial.resid = TRUE, pch=19, ylab= "linear")
plot(fit, se = FALSE, residuals = TRUE, pch=19, ylab = "K=35")
plot(fit4, se = FALSE, residuals = TRUE, pch=19, ylab = "cubic")
fit4 <- gam(accel ~ s(times, bs = "cr", k = round(df), fx=TRUE), data = dat)
plot(fit4, se = FALSE, residuals = TRUE, pch=19, ylab = "cubic")
fit4 <- gam(accel ~ s(times, bs = "cr", k = round(df), fx=TRUE), data = dat)
plot(fit4, se = FALSE, residuals = TRUE, pch=19, ylab = "cubic")
2.Use lm and poly to fit a polynomial to the data, with approximately the same degrees of freedom as was estimated by gam. Use termplot to plot the estimated polynomial and partial residuals. Note the substantially worse fit achieved by the polynomial, relative to the penalized regression spline fit.
5.Now plot the model residuals against time, and comment.
6.Fit a linear model including a b-spline using the function bs on times and select a suitable degree and the knots position. Compare this model with the previous ones and comment.
6.Fit a linear model including a b-spline using the function bs on times and select a suitable degree and the knots position. Compare this model with the previous ones and comment.
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(echo = TRUE)
#Load required libraries
library(MASS)
library(rpart)
library(rpart.plot)
#Load the dataset
url <- "http://stat4ds.rwth-aachen.de/data/Crabs.dat"
Crabs <- read.table(url, header = TRUE)
# Select variables of interest
data <- Crabs[, c("y", "weight", "color")]
# Build the classification tree
tree <- rpart(y ~ weight + color, data = data, method = "class")
knitr::opts_chunk$set(echo = TRUE)
plotcp(tree)
knitr::opts_chunk$set(echo = TRUE)
best_cp <- tree$cptable[which.min(tree$cptable[, "xerror"]), "CP"]
best_cp
knitr::opts_chunk$set(echo = TRUE)
pruned_tree <- prune(tree, cp = best_cp)
rpart.plot(pruned_tree, extra=1)
knitr::opts_chunk$set(echo = TRUE)
pruned_tree <- prune(tree, cp = best_cp)
rpart.plot(pruned_tree, extra=1)
knitr::opts_chunk$set(echo = TRUE)
print("Accuracy of the complex tree:", accuracy)
#Predictions on the entire dataset
predictions <- predict(tree, data, type = "class")
pruned_predictions <- predict(pruned_tree, data, type = "class")
# Calculate the prediction accuracy
accuracy <- mean(predictions == data$y)
pruned_accuracy <- mean(pruned_predictions == data$y)
# Prediction
result <- data.frame(
Actual = data$y,
FullTree_Predicted = predictions,
PrunedTree_Predicted = pruned_predictions
)
print("Accuracy of the complex tree:", accuracy)
knitr::opts_chunk$set(echo = TRUE)
print("Accuracy of the complex tree:")
print(accuracy)
knitr::opts_chunk$set(echo = TRUE)
print("Accuracy of the pruned tree:")
print(pruned_accuracy)
# Crabs predetti avere i satelliti con l'albero potato
predicted_satellites <- Crabs(result$PrunedTree_Predicted == 1)
knitr::opts_chunk$set(echo = TRUE)
# Crabs predetti avere i satelliti con l'albero potato
predicted_satellites <- sum(result$PrunedTree_Predicted == 1)
print("Crabs predetti avere i satelliti con l'albero potato:")
print(predicted_satellites)
# Nuova durata media delle chiamate
durata_media <- 4.5
# Generazione dei dati osservati con distribuzione esponenziale
lunghezza_chiamate <- rexp(15, rate = 1/durata_media)
lunghezza_chiamate
# Prior
prior_shape <- 4
prior_rate <- 2
# Simulazione della distribuzione a priori
prior_samples <- rgamma(10000, shape = prior_shape, rate = prior_rate)
# Verifica della distribuzione a priori
hist(prior_samples, breaks = 30, main = "Distribuzione a Priori", xlab = "Lambda")
# Likelihood
likelihood <- function(lambda, data) {
prod(dexp(data, rate = lambda))
}
# Posterior
posterior <- function(lambda, data, prior_shape, prior_rate) {
prior <- dgamma(lambda, shape = prior_shape, rate = prior_rate)
likelihood_val <- likelihood(lambda, data)
posterior_val <- prior * likelihood_val
return(posterior_val)
}
# Stima Bayesiana di lambda usando Metropolis-Hastings
metropolis_hastings <- function(data, prior_shape, prior_rate, iterations) {
lambda <- rep(1, iterations)
for (i in 2:iterations) {
candidate <- lambda[i-1] + rnorm(1, 0, 0.5)  # Proposta con distribuzione normale
acceptance_ratio <- posterior(candidate, data, prior_shape, prior_rate) / posterior(lambda[i-1], data, prior_shape, prior_rate)
# Accettazione o rifiuto della proposta
if (runif(1) < acceptance_ratio) {
lambda[i] <- candidate
} else {
lambda[i] <- lambda[i-1]
}
}
return(lambda)
}
# Eseguire l'algoritmo Metropolis-Hastings
iterations <- 10000
lambda_samples <- metropolis_hastings(lunghezza_chiamate, prior_shape, prior_rate, iterations)
setwd("/Users/andrea.buscema/Desktop/DSAI/Lectures/1 semester/Cloud/GIT/Cloud_project")
tinytex::tlmgr_install("bookmark")
tinytex::reinstall_tinytex(repository = "illinois")
update-tlmgr-latest(.sh/.exe) --update
